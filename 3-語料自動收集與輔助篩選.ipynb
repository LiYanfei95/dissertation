{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a48562",
   "metadata": {},
   "source": [
    "# 1 自動收集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63292035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox()  \n",
    "url = \"http://dh.ersjk.com/jsp/front/prodlist.jsp\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rname = '' # 賬號\n",
    "rpass = '' # 密碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af165d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.ID, \"rname\").click()\n",
    "driver.find_element(By.ID, \"rname\").send_keys(rname)\n",
    "driver.find_element(By.ID, \"rpass\").click()\n",
    "driver.find_element(By.ID, \"rpass\").send_keys(rpass)\n",
    "driver.find_element(By.LINK_TEXT, \"登 录\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98fa1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.CSS_SELECTOR, \".button:nth-child(1)\").click()\n",
    "driver.find_element(By.ID, \"selInput\").click()\n",
    "driver.find_element(By.CSS_SELECTOR, \"#jsfw > img\").click()\n",
    "driver.find_element(By.CSS_SELECTOR, \".button:nth-child(2)\").click()\n",
    "driver.find_element(By.CSS_SELECTOR, \".close\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddf0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_to_excel(keyword,file_path):\n",
    "    \n",
    "    #輸入檢索內容\n",
    "    input_keyword = driver.find_element(By.XPATH, '//*[@id=\"ikeyword\"]')\n",
    "    input_keyword.clear()\n",
    "    input_keyword.send_keys(keyword)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"iglju\"]/button').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #循環爬取頁面內容\n",
    "    df = pd.DataFrame()\n",
    "    page_num = driver.find_element(By.XPATH, '//*[@id=\"pagebox\"]/tbody/tr/td[1]/span[2]').text\n",
    "    n = int(''.join(filter(str.isdigit, page_num)))\n",
    "    lis = driver.find_elements(By.XPATH, '//*[@id=\"list_tb\"]/li')\n",
    "    if lis:\n",
    "        for i in range(n):\n",
    "            lis = driver.find_elements(By.XPATH, '//*[@id=\"list_tb\"]/li')\n",
    "            for li in lis:\n",
    "                bname = li.find_element(By.XPATH, './div[1]').text\n",
    "                volume = li.find_element(By.XPATH, './div[2]').text\n",
    "                text = li.find_element(By.XPATH, './div[3]').text\n",
    "                word = li.find_element(By.XPATH, './div[3]/span[1]').text\n",
    "                df = pd.concat([df, pd.DataFrame({'書名': [bname], '卷': [volume], '文本': [text], '詞彙': [word]})], ignore_index=True)\n",
    "            try:\n",
    "                next_page = driver.find_element(By.XPATH,'//*[@id=\"pagebox\"]/tbody/tr/td[1]/a[last()]')\n",
    "                next_page.click()\n",
    "            except NoSuchElementException:\n",
    "                pass #如果僅有一頁，就直接跳過點擊下一頁步驟\n",
    "\n",
    "            time.sleep(7)\n",
    "\n",
    "        #刪除文本中的雙引號\n",
    "        df['文本'] = df['文本'].str.replace('“', '')\n",
    "        df['文本'] = df['文本'].str.replace('”', '')\n",
    "        df = df.drop_duplicates() #刪除重複\n",
    "\n",
    "        df.to_excel(file_path, index=False)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9303a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = '秋老伏、進霑'.split('、')\n",
    "\n",
    "folder_path = r'F:\\古籍處理數據\\input\\繪圖'\n",
    "for keyword in keywords:\n",
    "    file_path = f'{folder_path}/{keyword}.xlsx'\n",
    "    scrape_data_to_excel(keyword,file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157e0ea",
   "metadata": {},
   "source": [
    "# 2 處理文本內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da89caa",
   "metadata": {},
   "source": [
    "標點、分詞、添加時空信息和提示並分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41d0d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import re\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipe = pipeline(\"token-classification\", model=r\"E:\\\\jupyter_notebook\\\\classical-chinese-punctuation-guwen-biaodian\", device=device)\n",
    "pipe2 = pipeline(\"token-classification\", model=r\"E:\\\\jupyter_notebook\\\\bert-base-chinese-ws\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e748a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義標點函數\n",
    "def punctuate_text(sample, batch_mode=False):\n",
    "    if batch_mode:\n",
    "        texts = sample['文本']\n",
    "    else:\n",
    "        texts = [sample['文本']]\n",
    "\n",
    "    punctuated_texts = pipe(texts)\n",
    "\n",
    "    def process_single_text(punctuated_text, original_text):\n",
    "        punctuation_dict = {p['start']: p['entity'] for p in punctuated_text}\n",
    "        punctuated = \"\".join(original_text[i] + punctuation_dict.get(i, '') for i in range(len(original_text)))\n",
    "        return punctuated\n",
    "\n",
    "    processed_texts = [process_single_text(punctuated_text, text) for punctuated_text, text in zip(punctuated_texts, texts)]\n",
    "    if batch_mode:\n",
    "        return {'文本': processed_texts}\n",
    "    else:\n",
    "        return {'文本': processed_texts[0]}\n",
    "\n",
    "# 定義分詞函數\n",
    "def word_segmentation(sample, batch_mode=False):\n",
    "    if batch_mode:\n",
    "        texts = sample['文本']\n",
    "    else:\n",
    "        texts = [sample['文本']]\n",
    "\n",
    "    results = pipe2(texts)\n",
    "\n",
    "    def process_single_text(result):\n",
    "        processed_text = ''.join([' ' + entry['word']\n",
    "                                  if entry['entity'] == 'B'\n",
    "                                  else entry['word']\n",
    "                                  for entry in result])\n",
    "        return processed_text.strip()\n",
    "\n",
    "    processed_texts = [process_single_text(result) for result in results]\n",
    "\n",
    "    if batch_mode:\n",
    "        return {'分詞': processed_texts}\n",
    "    else:\n",
    "        return {'分詞': processed_texts[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da7d143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xlsx_file(file_path):\n",
    "    print(f'正在處理的文件是{file_path}')\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # 讀取爲datasets\n",
    "    ds = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "    batch_mode = torch.cuda.is_available()\n",
    "    if batch_mode:\n",
    "        print('正在標點...')\n",
    "        batch_size = 16384\n",
    "        def add_punctuate_text(sample):\n",
    "            return punctuate_text(sample, batch_mode=True)\n",
    "        ds = ds.map(add_punctuate_text, batched=True, batch_size=batch_size)\n",
    "        print('正在分詞...')\n",
    "        def add_word_segmentation(sample):\n",
    "            return word_segmentation(sample, batch_mode=True)\n",
    "        ds = ds.map(add_word_segmentation, batched=True, batch_size=batch_size)\n",
    "\n",
    "    else:\n",
    "        print('正在標點...')\n",
    "        def add_punctuate_text_single(sample):\n",
    "            return punctuate_text(sample)\n",
    "        ds = ds.map(add_punctuate_text_single)\n",
    "        print('正在分詞...')\n",
    "        def add_word_segmentation_single(sample):\n",
    "            return word_segmentation(sample)\n",
    "        ds = ds.map(add_word_segmentation_single)\n",
    "\n",
    "    # 轉換回pandas DataFrame\n",
    "    df = ds.to_pandas()\n",
    "\n",
    "    # 添加時空信息\n",
    "    print('正在添加時空信息...')\n",
    "    df_1 = pd.read_excel(r'E:\\坚果云同步文件\\论文内容\\博士階段論文\\畢業論文\\論文數據\\愛如生數據庫書目（合併重複書名項、添加時代、索引年、坐標）.xlsx')\n",
    "\n",
    "    df1_dict = df_1.set_index('書名')[['時代作者', '版本', '索引年', '時間段', '地域','X', 'Y', 'sys_id', 'uri']].to_dict(orient='index')\n",
    "\n",
    "    def get_infor(x):\n",
    "        return df1_dict.get(x, {'時代作者': None, '版本': None, '索引年': None,'時間段': None, '地域': None, 'X': None, 'Y': None,  'sys_id': None, 'uri': None})\n",
    "\n",
    "    df[['時代作者', '版本', '索引年', '時間段', '地域','X', 'Y', 'sys_id', 'uri']] = df['書名'].apply(get_infor).apply(pd.Series)\n",
    "\n",
    "    #添加提示列\n",
    "    df['詞彙'] = df['詞彙'].str.replace(' ', '') # 刪除空格\n",
    "    keywords = df['詞彙'].unique().tolist() # 提取關鍵詞\n",
    "    spaced_keywords = []\n",
    "    for keyword in keywords: # 給關鍵詞添加空格\n",
    "        spaced_keyword = ' '.join(keyword)\n",
    "        spaced_keyword = ' ' + spaced_keyword + ' '\n",
    "        spaced_keywords.append(spaced_keyword)\n",
    "    keywords += spaced_keywords\n",
    "\n",
    "    df['提示'] = '可能非所需的詞彙'\n",
    "    for text in df['分詞']:\n",
    "        for keyword in keywords:\n",
    "            if keyword in text:\n",
    "                df.loc[df['分詞'] == text, '提示'] = None\n",
    "\n",
    "    # 用<span></span>標籤括住“文本”列中的關鍵詞\n",
    "    for keyword in keywords:\n",
    "        pattern = re.compile(re.escape(keyword))\n",
    "        df['文本'] = df['文本'].apply(lambda x: pattern.sub(f'<span>{keyword}</span>', x))\n",
    "\n",
    "    df = df.drop(['詞彙'], axis=1)\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f'文件已經保存在{file_path}')\n",
    "    return keywords # 返回keywords，以便後續使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0da38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def stopwords_list(file_path, keywords):\n",
    "    '''\n",
    "    file_path: 輸入以utf-8編碼的txt文件，文件以回車分隔停用詞;\n",
    "    keywords：將查詢的詞彙列表也當作停用詞，避免分類的時候受到干擾。\n",
    "    '''\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        stopwords = f.read().split('\\n')\n",
    "        new_keywords = keywords + list(''.join(keywords))\n",
    "        stopwords = stopwords + new_keywords + ['\\n', '\\r','\\r\\n']\n",
    "        stopwords = list(set(stopwords))\n",
    "    return stopwords\n",
    "\n",
    "def text_clustering(file_path, stop_words):\n",
    "    '''\n",
    "    如果語料大於300條，進行聚類以便人工閱讀。聚類數設置爲語料總數的1/50；\n",
    "    file_path: 需聚類的xlsx文件，其中有“分詞”列，是已分好詞的文本；\n",
    "    stop_words: 停用詞列表。\n",
    "    '''\n",
    "    df = pd.read_excel(file_path)\n",
    "    corpus = df['分詞']\n",
    "    \n",
    "    if len(corpus) <= 300:\n",
    "        print('語料內容不多，無需聚類。')\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words) # 創建TF-IDF向量化器，同時設置停用詞\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) # 將文本數據轉換成TF-IDF特征矩陣\n",
    "        # 使用K均值聚類\n",
    "        num_clusters = int(len(corpus) / 50)  # 將聚類數設置爲語料數的1/50\n",
    "        kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=42)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "\n",
    "        cluster_labels = kmeans.labels_ # 獲取聚類標籤\n",
    "        df['聚类标签'] = cluster_labels\n",
    "\n",
    "        #按照“聚類標籤”列的值進行分組，並且分組內按照“文本”列排序\n",
    "        df_sorted = df.groupby('聚类标签', group_keys=False).apply(lambda x: x.sort_values(by='文本'))\n",
    "        df_sorted = df_sorted.reset_index(drop=True) # 重置索引以获取最终的排序结果\n",
    "        \n",
    "        df_sorted.to_excel(file_path, index=False)\n",
    "        print('已聚類。')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce6071d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理的文件是F:\\古籍處理數據\\input\\繪圖\\官圍.xlsx\n",
      "正在標點...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分詞...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在添加時空信息...\n",
      "文件已經保存在F:\\古籍處理數據\\input\\繪圖\\官圍.xlsx\n",
      "已聚類。\n"
     ]
    }
   ],
   "source": [
    "file_path = r'F:\\古籍處理數據\\input\\繪圖\\官圍.xlsx'\n",
    "keywords = process_xlsx_file(file_path)\n",
    "stop_words = stopwords_list(r'F:\\古籍處理數據\\input\\繪圖\\停用詞.txt', keywords)\n",
    "text_clustering(file_path, stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
